{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import pybullet_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class hp():\n",
    "    def __init__(self):\n",
    "        self.n = 1000                                      #number of times we want to loop\n",
    "        self.episode_len = 1000                            #length of each episode from start to end\n",
    "        self.learn_rate = 0.02                             #learning rate\n",
    "        self.direction=16                                  #number of direction 16-positive 16-negative\n",
    "        self.best_direction = 16                           #best direction that support increasing reward\n",
    "        assert self.best_direction <= self.direction       #assertion\n",
    "        self.noise = 0.03                                  #standard deviation\n",
    "        self.seed = 1                                      #also called random state\n",
    "        self.env_name=\"HalfCheetahBulletEnv-v0\"            #environmnt name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Normalise():\n",
    "    def __init__(self,input_size):\n",
    "        self.n = np.zeros(input_size)\n",
    "        self.mean = np.zeros(input_size)\n",
    "        self.mean_diff = np.zeros(input_size)\n",
    "        self.var = np.zeros(input_size)\n",
    "    def observe(self,x):\n",
    "        self.n+=1\n",
    "        last_mean = self.mean.copy()\n",
    "        self.mean += (x-self.mean)/self.n\n",
    "        self.mean_diff += (x-last_mean)*(x-self.mean)\n",
    "        self.var = (self.mean_diff/self.n).clip(min=1e-2)\n",
    "    def normalise(self,inputs):\n",
    "        obs_mean = self.mean\n",
    "        obs_std = np.sqrt(self.var)\n",
    "        return (inputs-obs_mean)/obs_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    def __init__(self,input_size,output_size):\n",
    "        self.theta = np.zeros((output_size,input_size))             #weights matrix\n",
    "    \n",
    "    def evaluate(self,inputs,delta=None,direction=None):\n",
    "        if direction is None:\n",
    "            return (self.theta).dot(inputs)\n",
    "        elif direction == 'positive':\n",
    "            return (self.theta + hp.noise * delta).dot(inputs)        #perceptron\n",
    "        else:\n",
    "            return (self.theta - hp.noise * delta).dot(inputs)\n",
    "    def sample_deltas(self):\n",
    "        return [np.random.randn(*self.theta.shape) for _ in range(hp.direction)]\n",
    "    def update(self, rollouts , sigma_r):\n",
    "        step = np.zeros(self.theta.shape)\n",
    "        for rpos,rneg,d in rollouts:                                #approximated Gradient Descent\n",
    "            step+= (rpos-rneg)*d\n",
    "        self.theta+= hp.learn_rate/(hp.best_direction*sigma_r)*step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explore(env, normaliser, policy, direction=None , delta=None):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    num_plays = 0\n",
    "    sum_rewards = 0\n",
    "    while not done and num_plays<hp.episode_len:\n",
    "        normaliser.observe(state)\n",
    "        state = normaliser.normalise(state)\n",
    "        action = policy.evaluate(state,delta,direction)\n",
    "        state,reward,done,_ = env.step(action)\n",
    "        reward = max(min(reward,1),-1)\n",
    "        sum_rewards+= reward\n",
    "        num_plays+=1\n",
    "    return sum_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the A.I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(env, policy, normaliser, hp):\n",
    "    for i in range(hp.n):\n",
    "        \n",
    "            # Initialising deltas or pertubations for adjusting the weights\n",
    "            deltas = policy.sample_deltas()\n",
    "            positive_rewards = [0] * hp.direction\n",
    "            negative_rewards = [0] * hp.direction\n",
    "            \n",
    "            #Getting the positive rewards in positive direction\n",
    "            for k in range(hp.direction):\n",
    "                positive_rewards[k] = explore(env, normaliser, policy, direction=\"positive\", delta=deltas[k])\n",
    "                \n",
    "            #Getting the negative rewards in negative direction\n",
    "            for k in range(hp.direction):\n",
    "                negative_rewards[k] = explore(env, normaliser, policy, direction=\"negative\", delta=deltas[k])\n",
    "            \n",
    "            #Gathering all positive/negative rewards to compute the standard deviation of these rows\n",
    "            all_rewards = np.array(positive_rewards + negative_rewards)\n",
    "            sigma_r = all_rewards.std()\n",
    "            \n",
    "            #Sorting the rollouts by the max(rpos,rneg) and selecting the best directions\n",
    "            scores = { k:[max(r_pos,r_pos)]  for k,(r_pos,r_neg) in enumerate(zip(positive_rewards,negative_rewards))}\n",
    "            order = sorted(scores.keys(), key = lambda x: scores[x])[:hp.best_direction]\n",
    "            rollouts = [[positive_rewards[k],negative_rewards[k],deltas[k]] for k in order]\n",
    "            \n",
    "            #updating our policy\n",
    "            policy.update(rollouts,sigma_r)\n",
    "            \n",
    "            #Printing the final reward of the policy after the update\n",
    "            reward_evaluation = explore(env, normaliser, policy)\n",
    "            print('Step: ',i,' Reward :', reward_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mkdir(base,name):\n",
    "    path = os.path.join(base,name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "work_dir = mkdir('exp','brs')                                    #saving the videos of our A.I walking\n",
    "monitor_dir = mkdir(work_dir, 'monitor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "WalkerBase::__init__ start\n",
      "\u001b[33mWARN: Environment '<class 'pybullet_envs.gym_locomotion_envs.HalfCheetahBulletEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "Step:  0  Reward : -960.0487993108393\n",
      "Step:  1  Reward : -968.6851444661884\n",
      "Step:  2  Reward : -936.953898240947\n",
      "Step:  3  Reward : -937.1927955478343\n",
      "Step:  4  Reward : -935.8571926134282\n",
      "Step:  5  Reward : -908.5218897226828\n",
      "Step:  6  Reward : -955.3801738286089\n",
      "Step:  7  Reward : -939.6053974050517\n",
      "Step:  8  Reward : -923.8781642526792\n",
      "Step:  9  Reward : 493.1209579139428\n",
      "Step:  10  Reward : 495.3652840599653\n",
      "Step:  11  Reward : -512.5918281239182\n",
      "Step:  12  Reward : -393.8986278106682\n",
      "Step:  13  Reward : 461.0195078566046\n",
      "Step:  14  Reward : -869.6990014033564\n",
      "Step:  15  Reward : -549.8606254709465\n",
      "Step:  16  Reward : -701.1582012944989\n",
      "Step:  17  Reward : -653.9066417572069\n",
      "Step:  18  Reward : 2.7760914014794764\n",
      "Step:  19  Reward : -840.4384830232668\n",
      "Step:  20  Reward : -706.2993240710977\n",
      "Step:  21  Reward : -121.00631054699775\n",
      "Step:  22  Reward : -266.97626494039963\n",
      "Step:  23  Reward : -99.12905849261375\n",
      "Step:  24  Reward : -209.83328783545858\n",
      "Step:  25  Reward : -980.116999478518\n",
      "Step:  26  Reward : 489.4796432471419\n",
      "Step:  27  Reward : -740.4836037425445\n",
      "Step:  28  Reward : -181.54759786774085\n",
      "Step:  29  Reward : -291.17571127327557\n",
      "Step:  30  Reward : -69.41633751288317\n",
      "Step:  31  Reward : 94.05056430888477\n",
      "Step:  32  Reward : 7.6266043206779965\n",
      "Step:  33  Reward : -31.970706407556296\n",
      "Step:  34  Reward : -98.80941559650692\n",
      "Step:  35  Reward : 68.42217854681547\n",
      "Step:  36  Reward : 317.71124764497813\n",
      "Step:  37  Reward : 490.2385733702378\n",
      "Step:  38  Reward : 488.52235161991655\n",
      "Step:  39  Reward : 487.40420096227854\n",
      "Step:  40  Reward : 474.8206861628821\n",
      "Step:  41  Reward : -24.72380920333353\n",
      "Step:  42  Reward : 487.28519679269937\n",
      "Step:  43  Reward : 477.87817502998513\n",
      "Step:  44  Reward : 491.20693487351707\n",
      "Step:  45  Reward : 484.86908592030215\n",
      "Step:  46  Reward : 485.1000715433223\n",
      "Step:  47  Reward : 491.4033656791393\n",
      "Step:  48  Reward : 190.44016954501677\n",
      "Step:  49  Reward : 486.2411522260468\n",
      "Step:  50  Reward : 477.19031626212563\n",
      "Step:  51  Reward : -261.655890881221\n",
      "Step:  52  Reward : 485.2728712067685\n",
      "Step:  53  Reward : 207.08884718672022\n",
      "Step:  54  Reward : 487.5919036547421\n",
      "Step:  55  Reward : 478.67885118550777\n",
      "Step:  56  Reward : 470.3300185070032\n",
      "Step:  57  Reward : 490.7870845711141\n",
      "Step:  58  Reward : 487.63117158166693\n",
      "Step:  59  Reward : 485.5116596331405\n",
      "Step:  60  Reward : 487.89357321055206\n",
      "Step:  61  Reward : 469.72294745941866\n",
      "Step:  62  Reward : -97.64060829672769\n",
      "Step:  63  Reward : 490.8344678603314\n",
      "Step:  64  Reward : 486.80314662950263\n",
      "Step:  65  Reward : 485.73087148107516\n",
      "Step:  66  Reward : 485.16082193083713\n",
      "Step:  67  Reward : 488.11222449609284\n",
      "Step:  68  Reward : 485.0512188859771\n",
      "Step:  69  Reward : 486.76383895336613\n",
      "Step:  70  Reward : 485.9746783359271\n",
      "Step:  71  Reward : 481.3673699316932\n",
      "Step:  72  Reward : 485.43201269018067\n",
      "Step:  73  Reward : 488.7900650436136\n",
      "Step:  74  Reward : 490.28157046229865\n",
      "Step:  75  Reward : 358.51323262887274\n",
      "Step:  76  Reward : 489.7870740175127\n",
      "Step:  77  Reward : 486.29687247147075\n",
      "Step:  78  Reward : 467.67404492050815\n",
      "Step:  79  Reward : 466.1308773316182\n",
      "Step:  80  Reward : 482.17417201565337\n",
      "Step:  81  Reward : 486.2778677319128\n",
      "Step:  82  Reward : 50.65392278930199\n",
      "Step:  83  Reward : 435.7134810128501\n",
      "Step:  84  Reward : 484.6303931266896\n",
      "Step:  85  Reward : 465.94878016821554\n",
      "Step:  86  Reward : 479.72470968850945\n",
      "Step:  87  Reward : 485.51857973145013\n",
      "Step:  88  Reward : 481.2162515643473\n",
      "Step:  89  Reward : 470.8845367748955\n",
      "Step:  90  Reward : 484.58110864666685\n",
      "Step:  91  Reward : 483.50732380071645\n",
      "Step:  92  Reward : 486.82357575665833\n",
      "Step:  93  Reward : -183.95790785957354\n",
      "Step:  94  Reward : 482.3849299569147\n",
      "Step:  95  Reward : 489.99466171693837\n",
      "Step:  96  Reward : 467.6516649279916\n",
      "Step:  97  Reward : 487.9304518454027\n",
      "Step:  98  Reward : 461.52470826650955\n",
      "Step:  99  Reward : 479.00186515734754\n",
      "Step:  100  Reward : -111.93756125482412\n",
      "Step:  101  Reward : 489.1532223148179\n",
      "Step:  102  Reward : 474.98118088683117\n",
      "Step:  103  Reward : 491.3987868636141\n",
      "Step:  104  Reward : 487.86083669914734\n",
      "Step:  105  Reward : 484.14709765799995\n",
      "Step:  106  Reward : 484.7822186340528\n",
      "Step:  107  Reward : 465.32295044591706\n",
      "Step:  108  Reward : 488.934512414116\n",
      "Step:  109  Reward : 488.78539109216575\n",
      "Step:  110  Reward : 484.1359088281078\n",
      "Step:  111  Reward : 489.3431932741511\n",
      "Step:  112  Reward : 483.05859526533584\n",
      "Step:  113  Reward : 482.6765337731872\n",
      "Step:  114  Reward : 483.5444890662912\n",
      "Step:  115  Reward : 486.81196392954496\n",
      "Step:  116  Reward : 488.641960791504\n",
      "Step:  117  Reward : 485.381769467333\n",
      "Step:  118  Reward : 484.0443338106106\n",
      "Step:  119  Reward : 485.7317193459989\n",
      "Step:  120  Reward : 482.80716814235166\n",
      "Step:  121  Reward : 482.8398390590347\n",
      "Step:  122  Reward : 493.1796932361242\n",
      "Step:  123  Reward : 489.1702576256274\n",
      "Step:  124  Reward : 500.3196258429191\n",
      "Step:  125  Reward : 488.39554618447215\n",
      "Step:  126  Reward : 495.0791597963285\n",
      "Step:  127  Reward : 489.3543844044705\n",
      "Step:  128  Reward : 491.8817757056121\n",
      "Step:  129  Reward : 489.74796803407435\n",
      "Step:  130  Reward : 484.69761370297624\n",
      "Step:  131  Reward : 487.94281939969727\n",
      "Step:  132  Reward : 493.16735246749363\n",
      "Step:  133  Reward : 490.5434090813935\n",
      "Step:  134  Reward : 187.73564408843745\n",
      "Step:  135  Reward : 492.22658851916384\n",
      "Step:  136  Reward : 485.8045757192376\n",
      "Step:  137  Reward : 489.41371138711\n",
      "Step:  138  Reward : 491.23762894835204\n",
      "Step:  139  Reward : 491.5438136123613\n",
      "Step:  140  Reward : 409.7672239181676\n",
      "Step:  141  Reward : 492.4062523096498\n",
      "Step:  142  Reward : 491.5198769658205\n",
      "Step:  143  Reward : 491.4901710728626\n",
      "Step:  144  Reward : 390.8548588749872\n",
      "Step:  145  Reward : 490.6741511828398\n",
      "Step:  146  Reward : 489.876045912603\n",
      "Step:  147  Reward : 483.293129477509\n",
      "Step:  148  Reward : 395.57531903893454\n",
      "Step:  149  Reward : 491.7416531662661\n",
      "Step:  150  Reward : 492.46854651090086\n",
      "Step:  151  Reward : 246.26901137078346\n",
      "Step:  152  Reward : 403.84749289730104\n",
      "Step:  153  Reward : 281.77557153117186\n",
      "Step:  154  Reward : 361.5006011588653\n",
      "Step:  155  Reward : 511.39346105493934\n",
      "Step:  156  Reward : 505.3064978592265\n",
      "Step:  157  Reward : 486.7961954934441\n",
      "Step:  158  Reward : 503.41226320448857\n",
      "Step:  159  Reward : 509.1309457044128\n",
      "Step:  160  Reward : 508.24631861142024\n",
      "Step:  161  Reward : 489.71839057805124\n",
      "Step:  162  Reward : 498.36103265083887\n",
      "Step:  163  Reward : 485.82833955649005\n",
      "Step:  164  Reward : 484.00669877869007\n",
      "Step:  165  Reward : 489.44531680526177\n",
      "Step:  166  Reward : 491.27386243680195\n",
      "Step:  167  Reward : 486.93624463735324\n",
      "Step:  168  Reward : 488.94030767007587\n",
      "Step:  169  Reward : 508.6197048574691\n",
      "Step:  170  Reward : 488.9179014326827\n",
      "Step:  171  Reward : 514.6141948602439\n",
      "Step:  172  Reward : 487.31968192433175\n",
      "Step:  173  Reward : 473.1679979247371\n",
      "Step:  174  Reward : 431.2370911155367\n",
      "Step:  175  Reward : 493.5939204203552\n",
      "Step:  176  Reward : 492.68020313416145\n",
      "Step:  177  Reward : 490.1100144060972\n",
      "Step:  178  Reward : 489.08414882741107\n",
      "Step:  179  Reward : 491.5009794358589\n",
      "Step:  180  Reward : 490.0848266327908\n",
      "Step:  181  Reward : 484.9518377281532\n",
      "Step:  182  Reward : 490.58209392271795\n",
      "Step:  183  Reward : 492.169203269756\n",
      "Step:  184  Reward : 480.4553906944958\n",
      "Step:  185  Reward : 489.7678620213504\n",
      "Step:  186  Reward : 484.9205592156796\n",
      "Step:  187  Reward : 513.0729873650663\n",
      "Step:  188  Reward : 490.6510129823219\n",
      "Step:  189  Reward : 491.6194449543117\n",
      "Step:  190  Reward : 490.5889967715411\n",
      "Step:  191  Reward : 505.43580584249764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  192  Reward : 492.75549106137817\n",
      "Step:  193  Reward : 498.58590451844475\n",
      "Step:  194  Reward : 494.3528936269539\n",
      "Step:  195  Reward : 495.02299545492446\n",
      "Step:  196  Reward : 489.41403181937034\n",
      "Step:  197  Reward : 489.9133306491733\n",
      "Step:  198  Reward : 494.9378975638943\n",
      "Step:  199  Reward : 491.0268264012637\n",
      "Step:  200  Reward : 485.8098173070283\n",
      "Step:  201  Reward : 68.05330614585941\n",
      "Step:  202  Reward : 478.76452809203096\n",
      "Step:  203  Reward : 488.87255397341187\n",
      "Step:  204  Reward : 479.9809915918315\n",
      "Step:  205  Reward : 484.40609228345534\n",
      "Step:  206  Reward : 489.72090289284523\n",
      "Step:  207  Reward : 490.5366861375139\n",
      "Step:  208  Reward : 490.53854387372013\n",
      "Step:  209  Reward : 491.99912008534517\n",
      "Step:  210  Reward : 483.4483531984253\n",
      "Step:  211  Reward : 482.2188336722392\n",
      "Step:  212  Reward : 491.5337330094416\n",
      "Step:  213  Reward : 490.50238887457425\n",
      "Step:  214  Reward : 491.4881615961411\n",
      "Step:  215  Reward : -129.28078790044285\n",
      "Step:  216  Reward : 471.3540944235787\n",
      "Step:  217  Reward : 508.39152093722936\n",
      "Step:  218  Reward : 498.73802145936924\n",
      "Step:  219  Reward : 490.74226372093034\n",
      "Step:  220  Reward : 490.2353650865051\n",
      "Step:  221  Reward : 490.9371786770652\n",
      "Step:  222  Reward : 493.0829754530342\n",
      "Step:  223  Reward : 490.1584638531097\n",
      "Step:  224  Reward : 491.8125225621457\n",
      "Step:  225  Reward : 490.704880816643\n",
      "Step:  226  Reward : 489.7661476632909\n",
      "Step:  227  Reward : 491.15283827427777\n",
      "Step:  228  Reward : 490.97955242012205\n",
      "Step:  229  Reward : 492.462878085335\n",
      "Step:  230  Reward : 488.6950299019561\n",
      "Step:  231  Reward : 485.6095819675282\n",
      "Step:  232  Reward : 484.1048206352015\n",
      "Step:  233  Reward : 490.4084312488402\n",
      "Step:  234  Reward : 492.98837628863544\n",
      "Step:  235  Reward : 490.52743399557437\n",
      "Step:  236  Reward : 486.2235287296851\n",
      "Step:  237  Reward : 487.39081149177537\n",
      "Step:  238  Reward : 489.3859149734509\n",
      "Step:  239  Reward : 497.57523323253065\n",
      "Step:  240  Reward : 492.8988110127056\n",
      "Step:  241  Reward : 489.06102598100574\n",
      "Step:  242  Reward : 492.70191471793106\n",
      "Step:  243  Reward : 493.1941497876571\n",
      "Step:  244  Reward : 494.0672225212085\n",
      "Step:  245  Reward : 492.5820825435484\n",
      "Step:  246  Reward : 489.91145264652135\n",
      "Step:  247  Reward : 490.9713128508247\n",
      "Step:  248  Reward : 485.6456750917423\n",
      "Step:  249  Reward : 492.44127049019636\n",
      "Step:  250  Reward : 490.77506381911985\n",
      "Step:  251  Reward : 488.0785366420614\n",
      "Step:  252  Reward : 506.1741485554146\n",
      "Step:  253  Reward : 489.8213091535391\n",
      "Step:  254  Reward : 490.08320060750583\n",
      "Step:  255  Reward : 491.81612264735446\n",
      "Step:  256  Reward : 490.14774595317516\n",
      "Step:  257  Reward : 490.48432350354295\n",
      "Step:  258  Reward : 490.2721391394157\n",
      "Step:  259  Reward : 493.5858947455014\n",
      "Step:  260  Reward : 491.362536865214\n",
      "Step:  261  Reward : 490.9820826047007\n",
      "Step:  262  Reward : 492.1393743588588\n",
      "Step:  263  Reward : 584.1600200901701\n",
      "Step:  264  Reward : 488.02522530233944\n",
      "Step:  265  Reward : 489.55236661773034\n",
      "Step:  266  Reward : 493.22550098729914\n",
      "Step:  267  Reward : 491.4493020346696\n",
      "Step:  268  Reward : 489.8029212861395\n",
      "Step:  269  Reward : 492.4643692094569\n",
      "Step:  270  Reward : 492.48145109011125\n",
      "Step:  271  Reward : 491.9960762075801\n",
      "Step:  272  Reward : 487.7178863435807\n",
      "Step:  273  Reward : 486.47386616020805\n",
      "Step:  274  Reward : 490.5615106638705\n",
      "Step:  275  Reward : 516.5305007491579\n",
      "Step:  276  Reward : 491.5326903838538\n",
      "Step:  277  Reward : 497.9439080122647\n",
      "Step:  278  Reward : 521.436792196237\n",
      "Step:  279  Reward : 520.1631135359012\n",
      "Step:  280  Reward : 503.85933532963537\n",
      "Step:  281  Reward : 520.5786731616802\n",
      "Step:  282  Reward : 506.4725389652907\n",
      "Step:  283  Reward : 430.99665034215377\n",
      "Step:  284  Reward : 501.177353859482\n",
      "Step:  285  Reward : 512.7827935054391\n",
      "Step:  286  Reward : 506.6663960413358\n",
      "Step:  287  Reward : 508.31737658309885\n",
      "Step:  288  Reward : 530.9324105929728\n",
      "Step:  289  Reward : 525.4477125821296\n",
      "Step:  290  Reward : 530.8495586368703\n",
      "Step:  291  Reward : 516.6370149821255\n",
      "Step:  292  Reward : 582.3483322430366\n",
      "Step:  293  Reward : 532.2323953224148\n",
      "Step:  294  Reward : 581.9047045977375\n",
      "Step:  295  Reward : 514.3729074335986\n",
      "Step:  296  Reward : 583.3230274089057\n",
      "Step:  297  Reward : 587.477337368638\n",
      "Step:  298  Reward : 524.0172417906872\n",
      "Step:  299  Reward : 521.85552844931\n",
      "Step:  300  Reward : 545.4310924615572\n",
      "Step:  301  Reward : 525.4610245390546\n",
      "Step:  302  Reward : 582.492340309226\n",
      "Step:  303  Reward : 583.2129585481107\n",
      "Step:  304  Reward : 539.3518181644012\n",
      "Step:  305  Reward : 586.945495425694\n",
      "Step:  306  Reward : 552.6159329776278\n",
      "Step:  307  Reward : 550.5623100372584\n",
      "Step:  308  Reward : 534.3087549872744\n",
      "Step:  309  Reward : 558.0936945208595\n",
      "Step:  310  Reward : 581.1130575849116\n",
      "Step:  311  Reward : 590.6226995604173\n",
      "Step:  312  Reward : 538.5596967387135\n",
      "Step:  313  Reward : 579.8449068157188\n",
      "Step:  314  Reward : 557.8875819212008\n",
      "Step:  315  Reward : 390.9344335343176\n",
      "Step:  316  Reward : 549.548835316412\n",
      "Step:  317  Reward : 580.1238008455464\n",
      "Step:  318  Reward : 549.3237644304905\n",
      "Step:  319  Reward : 545.0047223468647\n",
      "Step:  320  Reward : 538.923807782478\n",
      "Step:  321  Reward : 584.0602737186207\n",
      "Step:  322  Reward : 553.9741980841294\n",
      "Step:  323  Reward : 542.5491628757592\n",
      "Step:  324  Reward : 544.3733146123702\n",
      "Step:  325  Reward : 573.2942704136983\n",
      "Step:  326  Reward : 582.801549912296\n",
      "Step:  327  Reward : 381.32506877180276\n",
      "Step:  328  Reward : 539.6574799452709\n",
      "Step:  329  Reward : 530.222095360427\n",
      "Step:  330  Reward : 529.9461001862961\n",
      "Step:  331  Reward : 543.2944255952503\n",
      "Step:  332  Reward : 590.2768072715841\n",
      "Step:  333  Reward : 590.5021160858344\n",
      "Step:  334  Reward : 581.1948329002427\n",
      "Step:  335  Reward : 482.8851152133309\n",
      "Step:  336  Reward : 583.5163549538814\n",
      "Step:  337  Reward : 579.8143410423588\n",
      "Step:  338  Reward : 556.3297362609935\n",
      "Step:  339  Reward : 580.7259585974351\n",
      "Step:  340  Reward : 555.6054318668553\n",
      "Step:  341  Reward : 582.381258782383\n",
      "Step:  342  Reward : 570.3701102886237\n",
      "Step:  343  Reward : 544.6896521540709\n",
      "Step:  344  Reward : 555.7171922251551\n",
      "Step:  345  Reward : 570.7509955782082\n",
      "Step:  346  Reward : 572.4631133672417\n",
      "Step:  347  Reward : 578.2206513773555\n",
      "Step:  348  Reward : 578.7216710525832\n",
      "Step:  349  Reward : 579.055549460976\n",
      "Step:  350  Reward : 573.332038189757\n",
      "Step:  351  Reward : 571.6335813882429\n",
      "Step:  352  Reward : 569.6514963165257\n",
      "Step:  353  Reward : 587.1107593961502\n",
      "Step:  354  Reward : 559.5013495437496\n",
      "Step:  355  Reward : 559.4797779858366\n",
      "Step:  356  Reward : 571.96278458335\n",
      "Step:  357  Reward : 579.1991868838955\n",
      "Step:  358  Reward : 581.5627881816546\n",
      "Step:  359  Reward : 573.5699257663382\n",
      "Step:  360  Reward : 576.1613993838708\n",
      "Step:  361  Reward : 567.7855346764222\n",
      "Step:  362  Reward : 568.7021482888197\n",
      "Step:  363  Reward : 564.4542005957243\n",
      "Step:  364  Reward : 591.8829458182723\n",
      "Step:  365  Reward : 583.8707655783783\n",
      "Step:  366  Reward : 590.5477933306677\n",
      "Step:  367  Reward : 590.0545467169014\n",
      "Step:  368  Reward : 569.3053147016635\n",
      "Step:  369  Reward : 576.7576145401338\n",
      "Step:  370  Reward : 582.8647593217803\n",
      "Step:  371  Reward : 595.6600082843984\n",
      "Step:  372  Reward : 587.4550581492731\n",
      "Step:  373  Reward : 582.9591551800811\n",
      "Step:  374  Reward : 585.8567488603015\n",
      "Step:  375  Reward : 636.0284116359702\n",
      "Step:  376  Reward : 602.2813518412369\n",
      "Step:  377  Reward : 583.3686018234818\n",
      "Step:  378  Reward : 678.6867422110836\n",
      "Step:  379  Reward : 614.2278757652716\n",
      "Step:  380  Reward : 583.1167830112266\n",
      "Step:  381  Reward : 585.2596146030595\n",
      "Step:  382  Reward : 585.1805645201154\n",
      "Step:  383  Reward : 607.6803353385953\n",
      "Step:  384  Reward : 580.231260239503\n",
      "Step:  385  Reward : 688.8082597937689\n",
      "Step:  386  Reward : 657.0578492701164\n",
      "Step:  387  Reward : 642.752877019337\n",
      "Step:  388  Reward : 680.361061720114\n",
      "Step:  389  Reward : 689.9350389068942\n",
      "Step:  390  Reward : 689.2855570528283\n",
      "Step:  391  Reward : 680.4780354011581\n",
      "Step:  392  Reward : 685.9050008746457\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c75718b2c64a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnb_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mnormaliser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNormalise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormaliser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-434915ba48f5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(env, policy, normaliser, hp)\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;31m#Getting the negative rewards in negative direction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[0mnegative_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexplore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormaliser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"negative\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;31m#Gathering all positive/negative rewards to compute the standard deviation of these rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-08b5ec8d8aa5>\u001b[0m in \u001b[0;36mexplore\u001b[1;34m(env, normaliser, policy, direction, delta)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormaliser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdirection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0msum_rewards\u001b[0m\u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pybullet_envs\\gym_locomotion_envs.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                 \u001b[0melectricity_cost\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melectricity_cost\u001b[0m  \u001b[1;33m*\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrobot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoint_speeds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# let's assume we have DC motor with controller, and reverse current braking\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m                 \u001b[0melectricity_cost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstall_torque_cost\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hp = hp()\n",
    "np.random.seed(hp.seed)\n",
    "env = gym.make(hp.env_name)\n",
    "env = wrappers.Monitor(env, monitor_dir , force = True)         #populates the videos to monitor_dir\n",
    "nb_inputs = env.observation_space.shape[0]\n",
    "nb_outputs = env.action_space.shape[0]\n",
    "policy = Policy(nb_inputs,nb_outputs)\n",
    "normaliser = Normalise(nb_inputs)\n",
    "train(env, policy, normaliser, hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
